
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Classification</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2009-10-06"><meta name="m-file" content="classdemo"><link rel="stylesheet" type="text/css" href="../../../matlab/demos/private/style.css"></head><body><div class="header"><div class="left"><a href="matlab:edit classdemo">Open classdemo.m in the Editor</a></div><div class="right"><a href="matlab:echodemo classdemo">Run in the Command Window</a></div></div><div class="content"><h1>Classification</h1><!--introduction--><p>Suppose you have a data set containing observations with measurements on different variables (called predictors) and their known class labels. If you obtain predictor values for new observations, could you determine to which classes those observations probably belong?  This is the problem of classification. This demo illustrates how to perform some classification algorithms in MATLAB&reg; using Statistics Toolbox&#8482; by applying them to Fisher's iris data.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Fisher's Iris Data</a></li><li><a href="#3">Linear and Quadratic Discriminant Analysis</a></li><li><a href="#15">NaiveBayes Classifier</a></li><li><a href="#19">Decision Tree</a></li><li><a href="#28">Conclusions</a></li></ul></div><h2>Fisher's Iris Data<a name="1"></a></h2><p>Fisher's iris data consists of measurements on the sepal length, sepal width, petal length, and petal width for 150 iris specimens.  There are 50 specimens from each of three species. Load the data and see how the sepal measurements differ between species. You can use the two columns containing sepal measurements.</p><pre class="codeinput">load <span class="string">fisheriris</span>
gscatter(meas(:,1), meas(:,2), species,<span class="string">'rgb'</span>,<span class="string">'osd'</span>);
xlabel(<span class="string">'Sepal length'</span>);
ylabel(<span class="string">'Sepal width'</span>);
N = size(meas,1);
</pre><img vspace="5" hspace="5" src="../classdemo_01.png" alt=""> <p>Suppose you measure a sepal and petal from an iris, and you need to determine its species on the basis of those measurements. One approach to solving this problem is known as discriminant analysis.</p><h2>Linear and Quadratic Discriminant Analysis<a name="3"></a></h2><p>The <tt>classify</tt> function can perform classification using different types of discriminant analysis. First classify the data using the default linear discriminant analysis (LDA).</p><pre class="codeinput">ldaClass = classify(meas(:,1:2),meas(:,1:2),species);
</pre><p>The observations with known class labels are usually called the training data. Now compute the resubstitution error, which is the misclassification error (the proportion of misclassified observations) on the training set.</p><pre class="codeinput">bad = ~strcmp(ldaClass,species);
ldaResubErr = sum(bad) / N
</pre><pre class="codeoutput">
ldaResubErr =

    0.2000

</pre><p>You can also compute the confusion matrix on the training set. A confusion matrix contains information about known class labels and predicted class labels. Generally speaking, the (i,j) element in the confusion matrix is the number of samples whose known class label is class i and whose predicted class is j.  The diagonal elements represent correctly classified observations.</p><pre class="codeinput">[ldaResubCM,grpOrder] = confusionmat(species,ldaClass)
</pre><pre class="codeoutput">
ldaResubCM =

    49     1     0
     0    36    14
     0    15    35


grpOrder = 

    'setosa'
    'versicolor'
    'virginica'

</pre><p>Of the 150 training observations, 20% or 30 observations are misclassified by the linear discriminant function. You can see which ones they are by drawing X through the misclassified points.</p><pre class="codeinput">hold <span class="string">on</span>;
plot(meas(bad,1), meas(bad,2), <span class="string">'kx'</span>);
hold <span class="string">off</span>;
</pre><img vspace="5" hspace="5" src="../classdemo_02.png" alt=""> <p>The function has separated the plane into regions divided by lines, and assigned different regions to different species.  One way to visualize these regions is to create a grid of (x,y) values and apply the classification function to that grid.</p><pre class="codeinput">[x,y] = meshgrid(4:.1:8,2:.1:4.5);
x = x(:);
y = y(:);
j = classify([x y],meas(:,1:2),species);
gscatter(x,y,j,<span class="string">'grb'</span>,<span class="string">'sod'</span>)
</pre><img vspace="5" hspace="5" src="../classdemo_03.png" alt=""> <p>For some data sets, the regions for the various classes are not well separated by lines. When that is the case, linear discriminant analysis is not appropriate. Instead, you can try quadratic discriminant analysis (QDA) for our data.</p><p>Compute the resubstitution error for quadratic discriminant analysis.</p><pre class="codeinput">qdaClass = classify(meas(:,1:2),meas(:,1:2),species,<span class="string">'quadratic'</span>);
bad = ~strcmp(qdaClass,species);
qdaResubErr = sum(bad) / N
</pre><pre class="codeoutput">
qdaResubErr =

    0.2000

</pre><p>You have computed the resubstitution error. Usually people are more interested in the test error (also referred to as generalization error), which is the expected prediction error on an independent set. In fact, the resubstitution error will likely under-estimate the test error.</p><p>In this case you don't have another labeled data set, but you can simulate one by doing cross-validation. A stratified 10-fold cross-validation is a popular choice for estimating the test error on classification algorithms. It randomly divides the training set into 10 disjoint subsets. Each subset has roughly equal size and roughly the same class proportions as in the training set. Remove one subset, train the classification model using the other nine subsets, and use the trained model to classify the removed subset. You could repeat this by removing each of the ten subsets one at a time.</p><p>Because cross-validation randomly divides data, its outcome depends on the initial random seed. To reproduce the exact results in this demo, execute the following command:</p><pre class="codeinput">s = RandStream(<span class="string">'mt19937ar'</span>,<span class="string">'seed'</span>,0);
RandStream.setDefaultStream(s);
</pre><p>First use <tt>cvpartition</tt> to generate 10 disjoint stratified subsets.</p><pre class="codeinput">cp = cvpartition(species,<span class="string">'k'</span>,10)
</pre><pre class="codeoutput">
cp = 

K-fold cross validation partition
             N: 150
   NumTestSets: 10
     TrainSize: 135  135  135  135  135  135  135  135  135  135
      TestSize: 15  15  15  15  15  15  15  15  15  15
</pre><p>The <tt>crossval</tt> function can estimate the misclassification error for both LDA and QDA using the given data partition <tt>cp</tt>.</p><p>Estimate the true test error for LDA using 10-fold stratified cross-validation.</p><pre class="codeinput">ldaClassFun= @(xtrain,ytrain,xtest)(classify(xtest,xtrain,ytrain));
ldaCVErr  = crossval(<span class="string">'mcr'</span>,meas(:,1:2),species,<span class="string">'predfun'</span>, <span class="keyword">...</span>
             ldaClassFun,<span class="string">'partition'</span>,cp)
</pre><pre class="codeoutput">
ldaCVErr =

    0.2000

</pre><p>The LDA cross-validation error has the same value as the LDA resubstitution error on this data.</p><p>Estimate the true test error for QDA using 10-fold stratified cross-validation.</p><pre class="codeinput">qdaClassFun = @(xtrain,ytrain,xtest)(classify(xtest,xtrain,ytrain,<span class="keyword">...</span>
              <span class="string">'quadratic'</span>));
qdaCVErr = crossval(<span class="string">'mcr'</span>,meas(:,1:2),species,<span class="string">'predfun'</span>,<span class="keyword">...</span>
           qdaClassFun,<span class="string">'partition'</span>,cp)
</pre><pre class="codeoutput">
qdaCVErr =

    0.2267

</pre><p>QDA has a slightly larger cross-validation error than LDA. It shows that a simpler model may get comparable, or better performance than a more complicated model.</p><h2>NaiveBayes Classifier<a name="15"></a></h2><p>The <tt>classify</tt> function has other two other types, 'diagLinear' and 'diagQuadratic'. They are similar to 'linear' and 'quadratic', but with diagonal covariance matrix estimates. These diagonal choices are specific examples of a Naive Bayes classifier, because they assume the variables are conditionally independent given the class label. Naive Bayes classifiers are among the most popular classifiers. While the assumption of class-conditional independence between variables is not true in general, Naive Bayes classifiers have been found to work well in practice on many data sets.</p><p>The <tt>NaiveBayes</tt> class can be used to create a more general type of Naive Bayes classifier.</p><p>First model each variable in each class using a Gaussian distribution. You can compute the resubstitution error and the cross-validation error.</p><pre class="codeinput">nbGau= NaiveBayes.fit(meas(:,1:2), species);
nbGauClass= nbGau.predict(meas(:,1:2));
bad = ~strcmp(nbGauClass,species);
nbGauResubErr = sum(bad) / N
nbGauClassFun = @(xtrain,ytrain,xtest)<span class="keyword">...</span>
               (predict(NaiveBayes.fit(xtrain,ytrain), xtest));
nbGauCVErr  = crossval(<span class="string">'mcr'</span>,meas(:,1:2),species,<span class="keyword">...</span>
              <span class="string">'predfun'</span>, nbGauClassFun,<span class="string">'partition'</span>,cp)
</pre><pre class="codeoutput">
nbGauResubErr =

    0.2200


nbGauCVErr =

    0.2200

</pre><p>So far you have assumed the variables from each class have a multivariate normal distribution. Often that is a reasonable assumption, but sometimes you may not be willing to make that assumption or you may see clearly that it is not valid.  Now try to model each variable in each class using a kernel density estimation, which is a more flexible nonparametric technique.</p><pre class="codeinput">nbKD= NaiveBayes.fit(meas(:,1:2), species,<span class="string">'dist'</span>,<span class="string">'kernel'</span>);
nbKDClass= nbKD.predict(meas(:,1:2));
bad = ~strcmp(nbKDClass,species);
nbKDResubErr = sum(bad) / N
nbKDClassFun = @(xtrain,ytrain,xtest)<span class="keyword">...</span>
            (predict(NaiveBayes.fit(xtrain,ytrain,<span class="string">'dist'</span>,<span class="string">'kernel'</span>),xtest));
nbKDCVErr = crossval(<span class="string">'mcr'</span>,meas(:,1:2),species,<span class="keyword">...</span>
            <span class="string">'predfun'</span>, nbKDClassFun,<span class="string">'partition'</span>,cp)
</pre><pre class="codeoutput">
nbKDResubErr =

    0.1800


nbKDCVErr =

    0.1933

</pre><p>For this data set, the Naive Bayes classifier with kernel density estimation gets smaller resubstitution error and cross-validation error than the Naive Bayes classifier with a Gaussian distribution.</p><h2>Decision Tree<a name="19"></a></h2><p>Another classification algorithm is based on a decision tree. A decision tree is a set of simple rules, such as "if the sepal length is less than 5.45, classify the specimen as setosa."  Decision trees are also nonparametric because they do not require any assumptions about the distribution of the variables in each class.</p><p>The <tt>classregtree</tt> class creates a decision tree. Create a decision tree for the iris data and see how well it classifies the irises into species.</p><pre class="codeinput">t = classregtree(meas(:,1:2), species,<span class="string">'names'</span>,{<span class="string">'SL'</span> <span class="string">'SW'</span> });
</pre><p>It's interesting to see how the decision tree method divides the plane. Use the same technique as above to visualize the regions assigned to each species.</p><pre class="codeinput">[grpname,node] = t.eval([x y]);
gscatter(x,y,grpname,<span class="string">'grb'</span>,<span class="string">'sod'</span>)
</pre><img vspace="5" hspace="5" src="../classdemo_04.png" alt=""> <p>Another way to visualize the decision tree is to draw a diagram of the decision rule and class assignments.</p><pre class="codeinput">view(t);
</pre><img vspace="5" hspace="5" src="../classdemo_05.png" alt=""> <p>This cluttered-looking tree uses a series of rules of the form "SL &lt; 5.45" to classify each specimen into one of 19 terminal nodes.  To determine the species assignment for an observation, start at the top node and apply the rule. If the point satisfies the rule you take the left path, and if not you take the right path. Ultimately you reach a terminal node that assigns the observation to one of the three species.</p><p>Compute the resubstitution error and the cross-validation error for decision tree.</p><pre class="codeinput">dtclass = t.eval(meas(:,1:2));
bad = ~strcmp(dtclass,species);
dtResubErr = sum(bad) / N

dtClassFun = @(xtrain,ytrain,xtest)(eval(classregtree(xtrain,ytrain),xtest));
dtCVErr  = crossval(<span class="string">'mcr'</span>,meas(:,1:2),species, <span class="keyword">...</span>
          <span class="string">'predfun'</span>, dtClassFun,<span class="string">'partition'</span>,cp)
</pre><pre class="codeoutput">
dtResubErr =

    0.1333


dtCVErr =

    0.3200

</pre><p>For the decision tree algorithm, the cross-validation error estimate is significantly larger than the resubstitution error. This shows that the generated tree overfits the training set. In other words, this is a tree that classifies the original training set well, but the structure of the tree is sensitive to this particular training set so that its performance on new data is likely to degrade. It is often possible to find a simpler tree that performs better than a more complex tree on new data.</p><p>Try pruning the tree. First compute the resubstitution error for various of subsets of the original tree. Then compute the cross-validation error for these sub-trees. A graph shows that the resubstitution error is overly optimistic. It always decreases as the tree size grows, but beyond a certain point, increasing the tree size increases the cross-validation error rate.</p><pre class="codeinput">resubcost = test(t,<span class="string">'resub'</span>);
[cost,secost,ntermnodes,bestlevel] = test(t,<span class="string">'cross'</span>,meas(:,1:2),species);
plot(ntermnodes,cost,<span class="string">'b-'</span>, ntermnodes,resubcost,<span class="string">'r--'</span>)
figure(gcf);
xlabel(<span class="string">'Number of terminal nodes'</span>);
ylabel(<span class="string">'Cost (misclassification error)'</span>)
legend(<span class="string">'Cross-validation'</span>,<span class="string">'Resubstitution'</span>)
</pre><img vspace="5" hspace="5" src="../classdemo_06.png" alt=""> <p>Which tree should you choose? A simple rule would be to choose the tree with the smallest cross-validation error.  While this may be satisfactory, you might prefer to use a simpler tree if it is roughly as good as a more complex tree. For this example, take the simplest tree that is within one standard error of the minimum.  That's the default rule used by the <tt>classregtree/test</tt> method.</p><p>You can show this on the graph by computing a cutoff value that is equal to the minimum cost plus one standard error.  The "best" level computed by the <tt>classregtree/test</tt> method is the smallest tree under this cutoff. (Note that bestlevel=0 corresponds to the unpruned tree, so you have to add 1 to use it as an index into the vector outputs from <tt>classregtree/test</tt>.)</p><pre class="codeinput">[mincost,minloc] = min(cost);
cutoff = mincost + secost(minloc);
hold <span class="string">on</span>
plot([0 20], [cutoff cutoff], <span class="string">'k:'</span>)
plot(ntermnodes(bestlevel+1), cost(bestlevel+1), <span class="string">'mo'</span>)
legend(<span class="string">'Cross-validation'</span>,<span class="string">'Resubstitution'</span>,<span class="string">'Min + 1 std. err.'</span>,<span class="string">'Best choice'</span>)
hold <span class="string">off</span>
</pre><img vspace="5" hspace="5" src="../classdemo_07.png" alt=""> <p>Finally, you can look at the pruned tree and compute the estimated misclassification error for it.</p><pre class="codeinput">pt = prune(t,bestlevel);
view(pt)
</pre><img vspace="5" hspace="5" src="../classdemo_08.png" alt=""> <pre class="codeinput">cost(bestlevel+1)
</pre><pre class="codeoutput">
ans =

    0.2400

</pre><h2>Conclusions<a name="28"></a></h2><p>This demonstration shows how to perform classification in MATLAB using Statistics Toolbox functions.</p><p>This demonstration is not meant to be an ideal analysis of the Fisher iris data, In fact, using the petal measurements instead of, or in addition to, the sepal measurements may lead to better classification. Also, this demonstration is not meant to compare the strengths and weaknesses of different classification algorithms. You may find it instructive to perform the analysis on other data sets and compare different algorithms. There are also Statistics Toolbox functions that implement other classification algorithms. For example, you can use <tt>TreeBagger</tt> to perform bootstrap aggregation for an ensemble of decision trees, as described in the example <a href="http://www.mathworks.com/access/helpdesk/help/toolbox/stats/br0gosr-1.html#br0g6t1-1">Classifying Radar Returns for Ionosphere Data</a>.</p><p class="footer">Copyright 2002-2009 The MathWorks, Inc.<br>
          Published with MATLAB&reg; 7.10</p><p class="footer" id="trademarks">MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks of their respective owners.</p></div><!--
##### SOURCE BEGIN #####
%% Classification
% Suppose you have a data set containing observations with measurements on
% different variables (called predictors) and their known class labels. If
% you obtain predictor values for new observations, could you determine to
% which classes those observations probably belong?  This is the problem of
% classification. This demo illustrates how to perform some classification
% algorithms in MATLAB(R) using Statistics Toolbox(TM) by applying them
% to Fisher's iris data.

%   Copyright 2002-2009 The MathWorks, Inc.
%   $Revision: 1.1.6.1.2.1 $  $Date: 2010/06/21 18:08:30 $

%% Fisher's Iris Data
% Fisher's iris data consists of measurements on the sepal length, sepal
% width, petal length, and petal width for 150 iris specimens.  There are
% 50 specimens from each of three species. Load the data and see how the
% sepal measurements differ between species. You can use the two columns
% containing sepal measurements.

load fisheriris
gscatter(meas(:,1), meas(:,2), species,'rgb','osd');
xlabel('Sepal length');
ylabel('Sepal width');
N = size(meas,1);
%%
% Suppose you measure a sepal and petal from an iris, and you need to
% determine its species on the basis of those measurements. One approach to
% solving this problem is known as discriminant analysis.

%% Linear and Quadratic Discriminant Analysis
% The |classify| function can perform classification using different types
% of discriminant analysis. First classify the data using the default
% linear discriminant analysis (LDA).

ldaClass = classify(meas(:,1:2),meas(:,1:2),species);

%%
% The observations with known class labels are usually called the training
% data. Now compute the resubstitution error, which is the
% misclassification error (the proportion of misclassified observations) on
% the training set.

bad = ~strcmp(ldaClass,species);
ldaResubErr = sum(bad) / N
%%
% You can also compute the confusion matrix on the training set. A
% confusion matrix contains information about known class labels and
% predicted class labels. Generally speaking, the (i,j) element in the
% confusion matrix is the number of samples whose known class label is
% class i and whose predicted class is j.  The diagonal elements represent
% correctly classified observations. 

[ldaResubCM,grpOrder] = confusionmat(species,ldaClass)
%%
% Of the 150 training observations, 20% or 30 observations are
% misclassified by the linear discriminant function. You can see which ones
% they are by drawing X through the misclassified points.

hold on;
plot(meas(bad,1), meas(bad,2), 'kx');
hold off;

%%
% The function has separated the plane into regions divided by lines, and
% assigned different regions to different species.  One way to visualize
% these regions is to create a grid of (x,y) values and apply the
% classification function to that grid.

[x,y] = meshgrid(4:.1:8,2:.1:4.5);
x = x(:);
y = y(:);
j = classify([x y],meas(:,1:2),species);
gscatter(x,y,j,'grb','sod')


%%
% For some data sets, the regions for the various classes are not well
% separated by lines. When that is the case, linear discriminant analysis
% is not appropriate. Instead, you can try quadratic discriminant analysis
% (QDA) for our data.
%
% Compute the resubstitution error for quadratic discriminant analysis.
qdaClass = classify(meas(:,1:2),meas(:,1:2),species,'quadratic');
bad = ~strcmp(qdaClass,species);
qdaResubErr = sum(bad) / N

%% 
% You have computed the resubstitution error. Usually people are more
% interested in the test error (also referred to as generalization error),
% which is the expected prediction error on an independent set. In fact,
% the resubstitution error will likely under-estimate the test error.
%
% In this case you don't have another labeled data set, but you can
% simulate one by doing cross-validation. A stratified 10-fold
% cross-validation is a popular choice for estimating the test error on
% classification algorithms. It randomly divides the training set into 10
% disjoint subsets. Each subset has roughly equal size and roughly the same
% class proportions as in the training set. Remove one subset, train the
% classification model using the other nine subsets, and use the trained
% model to classify the removed subset. You could repeat this by removing
% each of the ten subsets one at a time.
%
% Because cross-validation randomly divides data, its outcome depends on
% the initial random seed. To reproduce the exact results in this demo,
% execute the following command:

s = RandStream('mt19937ar','seed',0);
RandStream.setDefaultStream(s);

%%
% First use |cvpartition| to generate 10 disjoint stratified subsets.
cp = cvpartition(species,'k',10)
%%
% The |crossval| function can estimate the misclassification error for both
% LDA and QDA using the given data partition |cp|.
%
% Estimate the true test error for LDA using 10-fold stratified
% cross-validation.
ldaClassFun= @(xtrain,ytrain,xtest)(classify(xtest,xtrain,ytrain));
ldaCVErr  = crossval('mcr',meas(:,1:2),species,'predfun', ...
             ldaClassFun,'partition',cp)
%%
% The LDA cross-validation error has the same value as the LDA
% resubstitution error on this data.
%%
% Estimate the true test error for QDA using 10-fold stratified
% cross-validation.
qdaClassFun = @(xtrain,ytrain,xtest)(classify(xtest,xtrain,ytrain,...
              'quadratic'));
qdaCVErr = crossval('mcr',meas(:,1:2),species,'predfun',...
           qdaClassFun,'partition',cp)
%%
% QDA has a slightly larger cross-validation error than LDA. It shows that
% a simpler model may get comparable, or better performance than a more
% complicated model.

%% NaiveBayes Classifier
% The |classify| function has other two other types, 'diagLinear' and
% 'diagQuadratic'. They are similar to 'linear' and 'quadratic', but with
% diagonal covariance matrix estimates. These diagonal choices are specific
% examples of a Naive Bayes classifier, because they assume the variables
% are conditionally independent given the class label. Naive Bayes
% classifiers are among the most popular classifiers. While the assumption
% of class-conditional independence between variables is not true in
% general, Naive Bayes classifiers have been found to work well in practice
% on many data sets.  
%
% The |NaiveBayes| class can be used to create a more general type of
% Naive Bayes classifier.

%%
% First model each variable in each class using a Gaussian distribution.
% You can compute the resubstitution error and the cross-validation error.

nbGau= NaiveBayes.fit(meas(:,1:2), species);
nbGauClass= nbGau.predict(meas(:,1:2));
bad = ~strcmp(nbGauClass,species);
nbGauResubErr = sum(bad) / N
nbGauClassFun = @(xtrain,ytrain,xtest)...
               (predict(NaiveBayes.fit(xtrain,ytrain), xtest));        
nbGauCVErr  = crossval('mcr',meas(:,1:2),species,...
              'predfun', nbGauClassFun,'partition',cp)

%%
% So far you have assumed the variables from each class have a multivariate
% normal distribution. Often that is a reasonable assumption, but sometimes
% you may not be willing to make that assumption or you may see clearly
% that it is not valid.  Now try to model each variable in each class using
% a kernel density estimation, which is a more flexible nonparametric
% technique.

nbKD= NaiveBayes.fit(meas(:,1:2), species,'dist','kernel');
nbKDClass= nbKD.predict(meas(:,1:2));
bad = ~strcmp(nbKDClass,species);
nbKDResubErr = sum(bad) / N
nbKDClassFun = @(xtrain,ytrain,xtest)...
            (predict(NaiveBayes.fit(xtrain,ytrain,'dist','kernel'),xtest));
nbKDCVErr = crossval('mcr',meas(:,1:2),species,...
            'predfun', nbKDClassFun,'partition',cp)

%%
% For this data set, the Naive Bayes classifier with kernel density
% estimation gets smaller resubstitution error and cross-validation error
% than the Naive Bayes classifier with a Gaussian distribution.

%% Decision Tree
%
% Another classification algorithm is based on a decision tree. A decision
% tree is a set of simple rules, such as "if the sepal length is less than
% 5.45, classify the specimen as setosa."  Decision trees are also
% nonparametric because they do not require any assumptions about the
% distribution of the variables in each class.  
%
% The |classregtree| class creates a decision tree. Create a decision tree
% for the iris data and see how well it classifies the irises into
% species.

t = classregtree(meas(:,1:2), species,'names',{'SL' 'SW' });

%%
% It's interesting to see how the decision tree method divides the plane. 
% Use the same technique as above to visualize the regions assigned to each
% species.

[grpname,node] = t.eval([x y]);
gscatter(x,y,grpname,'grb','sod')

%%
% Another way to visualize the decision tree is to draw a diagram of the
% decision rule and class assignments.

view(t);
%%
% This cluttered-looking tree uses a series of rules of the form "SL <
% 5.45" to classify each specimen into one of 19 terminal nodes.  To
% determine the species assignment for an observation, start at the
% top node and apply the rule. If the point satisfies the rule you take
% the left path, and if not you take the right path. Ultimately you reach
% a terminal node that assigns the observation to one of the three species.
%

%% 
% Compute the resubstitution error and the cross-validation error for
% decision tree.
dtclass = t.eval(meas(:,1:2));
bad = ~strcmp(dtclass,species);
dtResubErr = sum(bad) / N

dtClassFun = @(xtrain,ytrain,xtest)(eval(classregtree(xtrain,ytrain),xtest));
dtCVErr  = crossval('mcr',meas(:,1:2),species, ...
          'predfun', dtClassFun,'partition',cp)
%%
% For the decision tree algorithm, the cross-validation error
% estimate is significantly larger than the resubstitution error. This
% shows that the generated tree overfits the training set. In other words,
% this is a tree that classifies the original training set well, but
% the structure of the tree is sensitive to this particular training set so
% that its performance on new data is likely to degrade. It is often
% possible to find a simpler tree that performs better than a more
% complex tree on new data.  
%
% Try pruning the tree. First compute the resubstitution error for various
% of subsets of the original tree. Then compute the cross-validation error
% for these sub-trees. A graph shows that the resubstitution error is
% overly optimistic. It always decreases as the tree size grows, but beyond
% a certain point, increasing the tree size increases the cross-validation
% error rate.

resubcost = test(t,'resub');
[cost,secost,ntermnodes,bestlevel] = test(t,'cross',meas(:,1:2),species);
plot(ntermnodes,cost,'b-', ntermnodes,resubcost,'rREPLACE_WITH_DASH_DASH')
figure(gcf);
xlabel('Number of terminal nodes');
ylabel('Cost (misclassification error)')
legend('Cross-validation','Resubstitution')

%%
% Which tree should you choose? A simple rule would be to choose the tree
% with the smallest cross-validation error.  While this may be
% satisfactory, you might prefer to use a simpler tree if it is roughly as
% good as a more complex tree. For this example, take the simplest tree
% that is within one standard error of the minimum.  That's the default
% rule used by the |classregtree/test| method.
%
% You can show this on the graph by computing a cutoff value that is equal
% to the minimum cost plus one standard error.  The "best" level computed
% by the |classregtree/test| method is the smallest tree under this cutoff.
% (Note that bestlevel=0 corresponds to the unpruned tree, so you have to
% add 1 to use it as an index into the vector outputs from
% |classregtree/test|.)

[mincost,minloc] = min(cost);
cutoff = mincost + secost(minloc);
hold on
plot([0 20], [cutoff cutoff], 'k:')
plot(ntermnodes(bestlevel+1), cost(bestlevel+1), 'mo')
legend('Cross-validation','Resubstitution','Min + 1 std. err.','Best choice')
hold off

%%
% Finally, you can look at the pruned tree and compute the estimated
% misclassification error for it. 

pt = prune(t,bestlevel);
view(pt)

%%
cost(bestlevel+1)

%% Conclusions
% This demonstration shows how to perform classification in MATLAB using
% Statistics Toolbox functions.
%
% This demonstration is not meant to be an ideal analysis of the Fisher
% iris data, In fact, using the petal measurements instead of, or in
% addition to, the sepal measurements may lead to better classification.
% Also, this demonstration is not meant to compare the strengths and
% weaknesses of different classification algorithms. You may find it
% instructive to perform the analysis on other data sets and compare
% different algorithms. There are also Statistics Toolbox functions that
% implement other classification algorithms. For example, you can use
% |TreeBagger| to perform bootstrap aggregation for an ensemble of decision
% trees, as described in the example 
% <http://www.mathworks.com/access/helpdesk/help/toolbox/stats/br0gosr-1.html#br0g6t1-1
% Classifying Radar Returns for Ionosphere Data>.

displayEndOfDemoMessage(mfilename)

##### SOURCE END #####
--></body></html>