
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- This HTML was auto-generated from MATLAB code. To make changes, update the MATLAB code and republish this document.       --><title>ロジスティック回帰モデルのベイズ解析</title><meta name="generator" content="MATLAB 7.11"><link rel="schema.DC" href="../http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2010-05-26"><meta name="DC.source" content="bayesdemo.m"><link rel="stylesheet" type="text/css" href="../../../../matlab/demos/private/style.css"></head><body><div class="header"><div class="left"><a href="matlab:edit bayesdemo">エディターで bayesdemo.m を開く</a></div><div class="right"><a href="matlab:echodemo bayesdemo">コマンド ウィンドウで実行</a></div></div><div class="content"><h1>ロジスティック回帰モデルのベイズ解析</h1><!--introduction--><p>統計的推定は、最尤推定 (MLE) に基づくのが一般的です。MLE はデータの尤度を最大化するパラメーターを選択するので、直観的に言えば魅力的です。MLE では、パラメーターは未知でも確定はしているものと想定されており、ある程度の信頼も持って推定されます。ベイズ解析では、未知のパラメーターについては確率を利用して数値化します。それゆえ未知のパラメーターは確立変数として定義されます。</p><p>このデモでは、Statistics Toolbox™ と関数 <tt>slicesample</tt> を使用して、ロジスティック回帰モデルでベイズの推論を行う方法を示します。</p><!--/introduction--><h2>目次</h2><div><ul><li><a href="#1">ベイズの推論</a></li><li><a href="#2">自動車実験データ</a></li><li><a href="#3">ロジスティック回帰モデル</a></li><li><a href="#8">スライス サンプリング</a></li><li><a href="#9">サンプラー出力の解析</a></li><li><a href="#15">モデル パラメーターの推定</a></li><li><a href="#19">まとめ</a></li></ul></div><h2>ベイズの推論<a name="1"></a></h2><p>ベイズの推論は、モデルまたはモデル パラメーターについての予備知識を取り込んで統計モデルを解析する処理です。このような推論の根底にあるのは、ベイズの定理です。</p><p><img src="../bayesdemo_eq01039.png" alt="$$P(\mathrm{parameters|data}) = \frac
{P(\mathrm{data|parameters}) \times P(\mathrm{parameters})}
{P(\mathrm{data})}
\propto \mathrm {likelihood} \times \mathrm{prior}$$"></p><p>たとえば、次のような正規の観測値があるとします。</p><p><img src="../bayesdemo_eq46115.png" alt="$$X|\theta \sim N(\theta, \sigma^2)$$"></p><p>ここで、sigma は既知であり、theta の予備知識は次のとおりです。</p><p><img src="../bayesdemo_eq59388.png" alt="$$\theta \sim N(\mu, \tau^2)$$"></p><p>この式で、「ハイパーパラメーター」と呼ばれることもある mu と tau も既知です。<tt>X</tt> の <tt>n</tt> 個の標本が観察される場合、次のような theta の事後分布を得ることができます。</p><p><img src="../bayesdemo_eq35829.png" alt="$$\theta|X \sim N\left(\frac{\tau^2}{\sigma^2/n + \tau^2} \bar X +
\frac{\sigma^2/n}{{\sigma^2/n + \tau^2}} \mu,
\frac{(\sigma^2/n)\times \tau^2}{\sigma^2/n +
\tau^2}\right)$$"></p><p>theta の事前分布、尤度分布、および事後分布を次のグラフに示します。</p><pre class="codeinput">rand(<span class="string">'state'</span>,0); randn(<span class="string">'state'</span>,0);

n = 20;
sigma = 50;
x = normrnd(10,sigma,n,1);
mu = 30;
tau = 20;
theta = linspace(-40, 100, 500);
y1 = normpdf(mean(x),theta,sigma/sqrt(n));
y2 = normpdf(theta,mu,tau);
postMean = tau^2*mean(x)/(tau^2+sigma^2/n) + sigma^2*mu/n/(tau^2+sigma^2/n);
postSD = sqrt(tau^2*sigma^2/n/(tau^2+sigma^2/n));
y3 = normpdf(theta, postMean,postSD);
plot(theta,y1,<span class="string">'-'</span>, theta,y2,<span class="string">'--'</span>, theta,y3,<span class="string">'-.'</span>)
legend(<span class="string">'Likelihood'</span>,<span class="string">'Prior'</span>,<span class="string">'Posterior'</span>)
xlabel(<span class="string">'\theta'</span>)
</pre><img vspace="5" hspace="5" src="../bayesdemo_01.png" alt=""> <h2>自動車実験データ<a name="2"></a></h2><p>前に示した正規平均推定の例などの単純な問題では、閉じた形での事後分布を理解するのは簡単です。しかし、非共役事前分布が関係する一般的な問題では、事後分布を解析的に計算するのは困難または不可能です。そこで、例としてロジスティック回帰を検討します。この例には、燃費テストで不合格になった、さまざまな重量の自動車の割合をモデル化するのに役立つ実験が含まれています。データには、重量、テストした自動車の台数、および不合格台数の観測値が含まれています。重量を変換して、回帰パラメーター推定における相関を低減させます。</p><pre class="codeinput"><span class="comment">% A set of car weights</span>
weight = [2100 2300 2500 2700 2900 3100 3300 3500 3700 3900 4100 4300]';
weight = (weight-2800)/1000;     <span class="comment">% recenter and rescale</span>
<span class="comment">% The number of cars tested at each weight</span>
total = [48 42 31 34 31 21 23 23 21 16 17 21]';
<span class="comment">% The number of cars that have poor mpg performances at each weight</span>
poor = [1 2 0 3 8 8 14 17 19 15 17 21]';
</pre><h2>ロジスティック回帰モデル<a name="3"></a></h2><p>ロジスティック回帰は、一般化線形モデルの特殊ケースであり、応答変数が二項なのでここで示したデータにとって適切です。ロジスティック回帰モデルを次のように記述することができます。</p><p><img src="../bayesdemo_eq47865.png" alt="$$P(\mathrm{failure}) = \frac{e^{Xb}}{1+e^{Xb}}$$"></p><p>ここで、X は設計行列で、b はモデル パラメーターを格納したベクトルです。MATLAB&reg; では、この式を次のように記述することができます。</p><pre class="codeinput">logitp = @(b,x) exp(b(1)+b(2).*x)./(1+exp(b(1)+b(2).*x));
</pre><p>予備知識または無情報事前分布がある場合には、モデル パラメーターの事前確率分布を指定することができます。たとえば、このデモでは、intercept パラメーター <tt>b1</tt> と slope パラメーター <tt>b2</tt> に正規事前分布を使用します。つまり、次のようになります。</p><pre class="codeinput">prior1 = @(b1) normpdf(b1,0,20);    <span class="comment">% prior for intercept</span>
prior2 = @(b2) normpdf(b2,0,20);    <span class="comment">% prior for slope</span>
</pre><p>ベイズの定理により、モデル パラメーターの結合事後分布は、尤度と事前分布の積に比例します。</p><pre class="codeinput">post = @(b) prod(binopdf(poor,total,logitp(b,weight))) <span class="keyword">...</span><span class="comment">% likelihood</span>
            * prior1(b(1)) * prior2(b(2));                  <span class="comment">% priors</span>
</pre><p>このモデルにおける事後分布の正規化定数は、解析的に処理しにくいことに注意してください。ただし、正規化定数が不明である場合でも、モデル パラメーターのおおよその範囲が既知であれば、事後分布を可視化することはできます。</p><pre class="codeinput">b1 = linspace(-2.5, -1, 50);
b2 = linspace(3, 5.5, 50);
simpost = zeros(50,50);
<span class="keyword">for</span> i = 1:length(b1)
    <span class="keyword">for</span> j = 1:length(b2)
        simpost(i,j) = post([b1(i), b2(j)]);
    <span class="keyword">end</span>;
<span class="keyword">end</span>;
mesh(b2,b1,simpost)
xlabel(<span class="string">'Slope'</span>)
ylabel(<span class="string">'Intercept'</span>)
zlabel(<span class="string">'Posterior density'</span>)
view(-110.30)
</pre><img vspace="5" hspace="5" src="../bayesdemo_02.png" alt=""> <p>この事後分布は、パラメーター空間内で対角線方向に延ばされています。これは、データを調べた結果、パラメーターが相関していると思われることを示しています。これは興味深いことです。なぜなら、データを収集する前は、独立しているものと想定していたからです。この相関は、事前分布と尤度関数を組み合わせたことに由来します。</p><h2>スライス サンプリング<a name="8"></a></h2><p>データのベイズ解析では、事後分布を集計するためにモンテ カルロ法がよく使用されます。その考え方は次のとおりです。事後分布を解析的に計算できない場合であっても、事後分布から無作為標本を生成して、そのランダムな値を使用して事後分布または導出統計量 (事後分布の平均値、中央値、標準偏差など) を推定することはできます。スライス サンプリングは、比例定数 (これはまさに、正規化定数が未知である解析しにくい事後分布から標本を抽出するために必要とされるものです) までしかわかっていないときに、任意の密度関数を使用して、分布から標本を抽出することを目的とするアルゴリズムの一種です。このアルゴリズムでは、独立した標本は生成されず、定常分布が対象の分布であるマルコフ系列が生成されます。このように、スライス サンプラーは、マルコフ連鎖モンテ カルロ (MCMC) アルゴリズムの一種です。ただし、スライス サンプラーは、よく知られている他の MCMC アルゴリズムとは異なります。指定する必要があるのは、スケーリングされた事後分布だけであるからです。提案分布と周辺分布を指定する必要はありません。</p><p>スライス サンプラーを燃費テストのロジスティック回帰モデルのベイズ解析の一部として使用する方法を次の例に示します。これには、モデル パラメーターの事後分布から無作為標本を生成し、サンプラーの出力を解析し、モデル パラメーターについて推定を行うことも含まれます。最初の手順は、無作為標本を生成することです。</p><pre class="codeinput">initial = [1 1];
nsamples = 1000;
trace = slicesample(initial,nsamples,<span class="string">'pdf'</span>,post,<span class="string">'width'</span>,[20 2]);
</pre><h2>サンプラー出力の解析<a name="9"></a></h2><p>スライス サンプラーから無作為標本を取得した後は、収束や混合などの問題を調査することが重要です。これは、対象の事後分布から得られる無作為の実現の集合の一部として標本を扱うことが理にかなうかをかどうかを判断するためです。出力を調べる最も簡単な方法は、周辺のトレース プロットを確認することです。</p><pre class="codeinput">subplot(2,1,1)
plot(trace(:0.1))
ylabel(<span class="string">'Intercept'</span>);
subplot(2,1,2)
plot(trace(:,2))
ylabel(<span class="string">'Slope'</span>);
xlabel(<span class="string">'Sample Number'</span>);
</pre><img vspace="5" hspace="5" src="../bayesdemo_03.png" alt=""> <p>これらのプロットから、50 件ほどの標本でパラメーターの初期値の影響が消えて、プロセスが変動しないように見えるまでにしばらく時間がかかることは明らかです。</p><p>また、収束の有無を確認する際に、移動ウィンドウを使用して統計量 (標本の平均値、中央値、標準偏差など) を計算することも役に立ちます。これにより、生のサンプル トレースより滑らかなプロットが得られるので、非定常性の識別と理解が簡単になります。</p><pre class="codeinput">movavg = filter( (1/50)*ones(50,1), 1, trace);
subplot(2,1,1)
plot(movavg(:0.1))
xlabel(<span class="string">'Number of samples'</span>)
ylabel(<span class="string">'Means of the intercept'</span>);
subplot(2,1,2)
plot(movavg(:,2))
xlabel(<span class="string">'Number of samples'</span>)
ylabel(<span class="string">'Means of the slope'</span>);
</pre><img vspace="5" hspace="5" src="../bayesdemo_04.png" alt=""> <p>これらは、反復回数 50 回の間の移動平均なので、最初の 50 個の値はプロットのその他の値とは比較できません。ただし、各プロットの残りは、パラメーターの事後平均が 100 回ほどの反復の後に定常性に収束したことを裏づけているように思われます。また、2 つのパラメーターが、事後密度の前のプロットに一致して相互に相関し合っていることも明らかです。</p><p>沈静化期間からは、対象の分布から得られる無作為の実現として理にかなう方法で処理できない標本しか得られないので、スライス サンプラーの最初の出力にある50 個ほどの値は使用しない方が良いでしょう。できることと言えば、その出力行を削除することくらいです。ただし、&quot;バーンイン&quot; 期間を指定することならできます。これは、適切なバーンイン長が (以前の実行例から) 既にわかっている場合に便利です。</p><pre class="codeinput">trace = slicesample(initial,nsamples,<span class="string">'pdf'</span>,post, <span class="keyword">...</span>
<span class="string">'width'</span>,[20 2],<span class="string">'burnin'</span>,50);
subplot(2,1,1)
plot(trace(:0.1))
ylabel(<span class="string">'Intercept'</span>);
subplot(2,1,2)
plot(trace(:,2))
ylabel(<span class="string">'Slope'</span>);
</pre><img vspace="5" hspace="5" src="../bayesdemo_05.png" alt=""> <p>これらのトレース プロットは、非定常性を示しているようには見えず、バーンイン期間が自己の仕事を完了したことを示しています。</p><p>ただし、トレース プロットには別の面があり、これも調査する必要があります。intercept パラメーターのトレースが高周波雑音のように見えるのに対して、slope パラメーターのトレースの周波数成分は比較的低いようです。これは、隣接する反復箇所で値間に自己相関があることを示します。この自己相関標本から平均値を計算することはできますが、標本内の冗長部分を取り除いて、必要とされるメモリ量を下げると都合が良い場合がよくあります。これにより自己相関が排除された場合には、これを複数の独立値で構成される標本として扱うこともできます。たとえば、10 個に 1 個の割合で 10 番目の値だけを残すことにより、標本を減らすことができます。</p><pre class="codeinput">trace = slicesample(initial,nsamples,<span class="string">'pdf'</span>,post,<span class="string">'width'</span>,[20 2], <span class="keyword">...</span>
<span class="string">'burnin'</span>,50,<span class="string">'thin'</span>,10);
</pre><p>この間引きの効果を確認するには、標本の自己相関関数をトレースから推定し、その関数を使用して標本がすぐに混合するかどうかを確認します。</p><pre class="codeinput">F    =  fft(detrend(trace,<span class="string">'constant'</span>));
F    =  F .* conj(F);
ACF  =  ifft(F);
ACF  =  ACF(1:21,:);                          <span class="comment">% Retain lags up to 20.</span>
ACF  =  real([ACF(1:21,1) ./ ACF(1,1) <span class="keyword">...</span>
             ACF(1:21,2) ./ ACF(1,2)]);       <span class="comment">% Normalize.</span>
bounds  =  sqrt(1/nsamples) * [2 ; -2];       <span class="comment">% 95% CI for iid normal</span>

labs = {<span class="string">'Sample ACF for intercept'</span>,<span class="string">'Sample ACF for slope'</span> };
<span class="keyword">for</span> i = 1:2
    subplot(2,1,i)
    lineHandles  =  stem(0:20, ACF(:,i) , <span class="string">'filled'</span> , <span class="string">'r-o'</span>);
    set(lineHandles , <span class="string">'MarkerSize'</span> , 4)
    grid(<span class="string">'on'</span>)
    xlabel(<span class="string">'Lag'</span>)
    ylabel(labs{i})
    hold(<span class="string">'on'</span>)
    plot([0.5 0.5 ; 20 20] , [bounds([1 1]) bounds([2 2])] , <span class="string">'-b'</span>);
    plot([0 20] , [0 0] , <span class="string">'-k'</span>);
    hold(<span class="string">'off'</span>)
    a  =  axis;
    axis([a(1:3) 1]);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="../bayesdemo_06.png" alt=""> <p>最初のラグにおける自己相関値は、intercept パラメーターにとって重要ですが、slope パラメーターにとってはさらに重要です。相関度をさらに引き下げるため、より大きい間引きパラメーターを使用してサンプリングを繰り返すことができます。ただし、このデモには現在の標本を引き続き使用します。</p><h2>モデル パラメーターの推定<a name="15"></a></h2><p>予想どおり、標本のヒストグラムは事後密度のプロットによく似ています。</p><pre class="codeinput">subplot(1,1,1)
hist3(trace,[25,25]);
xlabel(<span class="string">'Intercept'</span>)
ylabel(<span class="string">'Slope'</span>)
zlabel(<span class="string">'Posterior density'</span>)
view(-110.30)
</pre><img vspace="5" hspace="5" src="../bayesdemo_07.png" alt=""> <p>ヒストグラムまたはカーネルの平滑化密度推定を使用して、事後標本の周辺分布プロパティを集計することができます。</p><pre class="codeinput">subplot(2,1,1)
hist(trace(:,1))
xlabel(<span class="string">'Intercept'</span>);
subplot(2,1,2)
ksdensity(trace(:,2))
xlabel(<span class="string">'Slope'</span>);
</pre><img vspace="5" hspace="5" src="../bayesdemo_08.png" alt=""> <p>また、記述統計量 (事後平均値や無作為標本のパーセンタイルなど) を計算することもできます。標本サイズが目的の精度を得るのに十分な大きさかどうかを判断するには、標本数の関数としてトレースの目的の統計量を監視します。</p><pre class="codeinput">csum = cumsum(trace);
subplot(2,1,1)
plot(csum(:0.1)'./(1:nsamples))
xlabel(<span class="string">'Number of samples'</span>)
ylabel(<span class="string">'Means of the intercept'</span>);
subplot(2,1,2)
plot(csum(:,2)'./(1:nsamples))
xlabel(<span class="string">'Number of samples'</span>)
ylabel(<span class="string">'Means of the slope'</span>);
</pre><img vspace="5" hspace="5" src="../bayesdemo_09.png" alt=""> <p>このケースでは、良好な精度の事後平均値推定を得るために、標本サイズ 1,000 は十分すぎるほどの大きさがあるようです。</p><pre class="codeinput">bHat = mean(trace)
</pre><pre class="codeoutput">
bHat =

   -1.6937    4.2764

</pre><h2>まとめ<a name="19"></a></h2><p>Statistics Toolbox にはさまざまな関数があり、尤度と先験的確率を簡単に指定することができます。それらの関数を組み合わせて、1 つの事後分布を導き出すことができます。関数 <tt>slicesample</tt> を使用すると、マルコフ連鎖モンテ カルロ シミュレーションを使用して MATLAB でベイズ解析を実行することができます。この関数は、標準の乱数発生器を使用して標本を抽出するのが難しい事後分布に関する問題にも使用することができます。</p><p class="footer">Copyright 2005-2008 The MathWorks, Inc.<br>Published with MATLAB&reg; 7.11</p><p class="footer" id="trademarks">MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks of their respective owners.</p></div><!-- ##### SOURCE BEGIN ##### %% Bayesian Analysis for a Logistic Regression Model % Statistical inferences are usually based on  maximum likelihood % estimation (MLE). MLE chooses the parameters that maximize the likelihood % of the data, and is intuitively appealing. In MLE, parameters are assumed % to be unknown but fixed, and are estimated with some confidence. In % Bayesian statistics, the uncertainty about the unknown parameters is % quantified using probability so that the unknown parameters are regarded % as random variables.  % % This demo illustrates the use of the Statistics Toolbox(TM) and the % |slicesample| function to make Bayesian inferences for a logistic % regression model.  %   Copyright 2005-2008 The MathWorks, Inc. %   $Revision: 1.1.4.2.2.1 $  $Date: 2010/07/29 21:29:28 $  %% Bayesian Inference % Bayesian inference is the process of analyzing statistical models with % the incorporation of prior knowledge about the model or model parameters. % The root of a such inference is Bayes' theorem: % % $$P(\mathrm{parameters|data}) = \frac %             {P(\mathrm{data|parameters}) \times P(\mathrm{parameters})} %             {P(\mathrm{data})} %             \propto \mathrm {likelihood} \times \mathrm{prior}$$ % % For example, suppose we have normal observations % % $$X|\theta \sim N(\theta, \sigma^2)$$ % % where sigma is known and the prior distribution for theta is  % % $$\theta \sim N(\mu, \tau^2)$$ % % In this formula mu and tau, sometimes known as hyperparameters, are also % known.  If we observe |n| samples of |X|, we can obtain the posterior % distribution for theta as  % % $$\theta|X \sim N\left(\frac{\tau^2}{\sigma^2/n + \tau^2} \bar X + %                   \frac{\sigma^2/n}{{\sigma^2/n + \tau^2}} \mu, %                   \frac{(\sigma^2/n)\times \tau^2}{\sigma^2/n + %                   \tau^2}\right)$$ % % The following graph shows the prior, likelihood, and posterior for theta. rand('state',0); randn('state',0);  n = 20; sigma = 50; x = normrnd(10,sigma,n,1); mu = 30; tau = 20; theta = linspace(-40, 100, 500); y1 = normpdf(mean(x),theta,sigma/sqrt(n)); y2 = normpdf(theta,mu,tau); postMean = tau^2*mean(x)/(tau^2+sigma^2/n) + sigma^2*mu/n/(tau^2+sigma^2/n); postSD = sqrt(tau^2*sigma^2/n/(tau^2+sigma^2/n)); y3 = normpdf(theta, postMean,postSD); plot(theta,y1,'-', theta,y2,'REPLACE_WITH_DASH_DASH', theta,y3,'-.') legend('Likelihood','Prior','Posterior') xlabel('\theta')   %% Car Experiment Data % In some simple problems such as the previous normal mean inference % example, it is easy to figure out the posterior distribution in a closed % form. But in general problems that involve non-conjugate priors, the % posterior distributions are difficult or impossible to compute % analytically. We will consider logistic regression as an example. This % example involves an experiment to help model the proportion of cars of % various weights that fail a mileage test. The data include observations % of weight, number of cars tested, and number failed.  We will work with % a transformed version of the weights to reduce the correlation in our % estimates of the regression parameters.   % A set of car weights weight = [2100 2300 2500 2700 2900 3100 3300 3500 3700 3900 4100 4300]'; weight = (weight-2800)/1000;     % recenter and rescale % The number of cars tested at each weight total = [48 42 31 34 31 21 23 23 21 16 17 21]'; % The number of cars that have poor mpg performances at each weight poor = [1 2 0 3 8 8 14 17 19 15 17 21]';   %% Logistic Regression Model  % Logistic regression, a special case of a generalized linear model,  % is appropriate for these data since the response variable is binomial. % The logistic regression model can be written as: %  % $$P(\mathrm{failure}) = \frac{e^{Xb}}{1+e^{Xb}}$$ %  % where X is the design matrix and b is the vector containing the model % parameters. In MATLAB(R), we can write this equation as: logitp = @(b,x) exp(b(1)+b(2).*x)./(1+exp(b(1)+b(2).*x));  %% % If you have some prior knowledge or some non-informative priors are % available, you could specify the prior probability distributions for the % model parameters. For example, in this demo, we use normal priors for the % intercept |b1| and slope |b2|, i.e. prior1 = @(b1) normpdf(b1,0,20);    % prior for intercept  prior2 = @(b2) normpdf(b2,0,20);    % prior for slope  %% % By Bayes' theorem, the joint posterior distribution of the model parameters % is proportional to the product of the likelihood and priors.  post = @(b) prod(binopdf(poor,total,logitp(b,weight))) ...  % likelihood             * prior1(b(1)) * prior2(b(2));                  % priors          %% % Note that the normalizing constant for the posterior in this model is % analytically intractable.  However, even without knowing the normalizing % constant, you can visualize the posterior distribution, if you know the % approximate range of the model parameters. b1 = linspace(-2.5, -1, 50); b2 = linspace(3, 5.5, 50); simpost = zeros(50,50); for i = 1:length(b1)     for j = 1:length(b2)         simpost(i,j) = post([b1(i), b2(j)]);     end; end; mesh(b2,b1,simpost) xlabel('Slope') ylabel('Intercept') zlabel('Posterior density') view(-110,30)  %% % This posterior is elongated along a diagonal in the parameter space, % indicating that, after we look at the data, we believe that the % parameters are correlated.  This is interesting, since before we % collected any data we assumed they were independent.  The correlation % comes from combining our prior distribution with the likelihood function.   %% Slice Sampling % Monte Carlo methods are often used in Bayesian data analysis to summarize % the posterior distribution. The idea is that, even if you cannot compute % the posterior distribution analytically, you can generate a random sample % from the distribution and use these random values to estimate the posterior % distribution or derived statistics such as the posterior mean, median, % standard deviation, etc. Slice sampling is an algorithm designed to sample % from a distribution with an arbitrary density function, known only up to a % constant of proportionality REPLACE_WITH_DASH_DASH exactly what is needed for sampling from a % complicated posterior distribution whose normalization constant is unknown. % The algorithm does not generate independent samples, but rather a Markovian % sequence whose stationary distribution is the target distribution.  Thus, % the slice sampler is a Markov Chain Monte Carlo (MCMC) algorithm.  However, % it differs from other well-known MCMC algorithms because only the scaled % posterior need be specified REPLACE_WITH_DASH_DASH no proposal or marginal distributions are % needed. % % This example shows how to use the slice sampler as part of a Bayesian % analysis of the mileage test logistic regression model, including generating % a random sample from the posterior distribution for the model parameters, % analyzing the output of the sampler, and making inferences about the model % parameters.  The first step is to generate a random sample. initial = [1 1]; nsamples = 1000; trace = slicesample(initial,nsamples,'pdf',post,'width',[20 2]);   %% Analysis of Sampler Output % After obtaining a random sample from the slice sampler, it is important to % investigate issues such as convergence and mixing, to determine whether the % sample can reasonably be treated as a set of random realizations from the % target posterior distribution. Looking at marginal trace plots is the % simplest way to examine the output. subplot(2,1,1) plot(trace(:,1)) ylabel('Intercept'); subplot(2,1,2) plot(trace(:,2)) ylabel('Slope'); xlabel('Sample Number');  %% % It is apparent from these plots is that the effects of the parameter % starting values take a while to disappear (perhaps fifty or so samples) % before the process begins to look stationary. % % It is also be helpful in checking for convergence to use a moving window % to compute statistics such as the sample mean, median, or standard % deviation for the sample.  This produces a smoother plot than the raw % sample traces, and can make it easier to identify and understand any % non-stationarity. movavg = filter( (1/50)*ones(50,1), 1, trace); subplot(2,1,1) plot(movavg(:,1)) xlabel('Number of samples') ylabel('Means of the intercept'); subplot(2,1,2) plot(movavg(:,2)) xlabel('Number of samples') ylabel('Means of the slope');  %% % Because these are moving averages over a window of 50 iterations, the first % 50 values are not comparable to the rest of the plot.  However, the % remainder of each plot seems to confirm that the parameter posterior means % have converged to stationarity after 100 or so iterations.  It is also % apparent that the two parameters are correlated with each other, in % agreement with the earlier plot of the posterior density. % % Since the settling-in period represents samples that can not reasonably be % treated as random realizations from the target distribution, it's probably % advisable not to use the first 50 or so values at the beginning of the % slice sampler's output.  You could just delete those rows of the output, % however, it's also possible to specify a "burn-in" period.  This is % convenient when a suitable burn-in length is already known, perhaps from % previous runs. trace = slicesample(initial,nsamples,'pdf',post, ...                                      'width',[20 2],'burnin',50); subplot(2,1,1) plot(trace(:,1)) ylabel('Intercept'); subplot(2,1,2) plot(trace(:,2)) ylabel('Slope');  %% % These trace plots do not seem to show any non-stationarity, indicating that % the burn-in period has done its job. % % However, there is a second aspect of the trace plots that should also be % explored.  While the trace for the intercept looks like high frequency % noise, the trace for the slope appears to have a lower frequency % component, indicating there autocorrelation between values at adjacent % iterations.  We could still compute the mean from this autocorrelated % sample, but it is often convenient to reduce the storage requirements by % removing redundancy in the sample.  If this eliminated the % autocorrelation, it would also allow us to treat this as a sample of % independent values.  For example, you can thin out the sample by keeping % only every 10th value. trace = slicesample(initial,nsamples,'pdf',post,'width',[20 2], ...                                                 'burnin',50,'thin',10);  %% % To check the effect of this thinning, it is useful to estimate the sample % autocorrelation functions from the traces and use them to check if the % samples mix rapidly. F    =  fft(detrend(trace,'constant')); F    =  F .* conj(F); ACF  =  ifft(F); ACF  =  ACF(1:21,:);                          % Retain lags up to 20. ACF  =  real([ACF(1:21,1) ./ ACF(1,1) ...              ACF(1:21,2) ./ ACF(1,2)]);       % Normalize. bounds  =  sqrt(1/nsamples) * [2 ; -2];       % 95% CI for iid normal  labs = {'Sample ACF for intercept','Sample ACF for slope' }; for i = 1:2     subplot(2,1,i)     lineHandles  =  stem(0:20, ACF(:,i) , 'filled' , 'r-o');     set(lineHandles , 'MarkerSize' , 4)     grid('on')     xlabel('Lag')     ylabel(labs{i})     hold('on')     plot([0.5 0.5 ; 20 20] , [bounds([1 1]) bounds([2 2])] , '-b');     plot([0 20] , [0 0] , '-k');     hold('off')     a  =  axis;     axis([a(1:3) 1]); end  %% % The autocorrelation values at the first lag are significant for the % intercept parameter, and even more so for the slope parameter.  We could % repeat the sampling using a larger thinning parameter in order to reduce % the correlation further.  For the purposes of this demo, however, we'll % continue to use the current sample.  %% Inference for the Model Parameters % As expected, a histogram of the sample mimics the plot of the posterior % density. subplot(1,1,1) hist3(trace,[25,25]); xlabel('Intercept') ylabel('Slope') zlabel('Posterior density') view(-110,30)  %% % You can use a histogram or a kernel smoothing density estimate to summarize % the marginal distribution properties of the posterior samples. subplot(2,1,1) hist(trace(:,1)) xlabel('Intercept'); subplot(2,1,2) ksdensity(trace(:,2)) xlabel('Slope');  %% % You could also compute descriptive statistics such as the posterior mean or % percentiles from the random samples. To determine if the sample size is % large enough to achieve a desired precision, it is helpful to monitor the  % desired statistic of the traces as a function of the number of samples. csum = cumsum(trace); subplot(2,1,1) plot(csum(:,1)'./(1:nsamples)) xlabel('Number of samples') ylabel('Means of the intercept'); subplot(2,1,2) plot(csum(:,2)'./(1:nsamples)) xlabel('Number of samples') ylabel('Means of the slope'); %% % In this case, it appears that the sample size of 1000 is more than % sufficient to give good precision for the posterior mean estimate. bHat = mean(trace)  %% Summary  % The Statistics Toolbox offers a variety of functions that allow you to % specify likelihoods and priors easily.  They can be combined to  % derive a posterior distribution.  The |slicesample| function enables % you to carry out Bayesian analysis in MATLAB using Markov Chain Monte % Carlo simulation.  It can be used even in problems with posterior % distributions that are difficult to sample from using standard random % number generators.  displayEndOfDemoMessage(mfilename)  ##### SOURCE END ##### --></body></html>